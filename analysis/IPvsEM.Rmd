---
title: "A brief comparison of speed of Interior Point and EM algorithms"
author: "Matthew Stephens"
date: 2016-01-26
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=FALSE}
source("chunk-options.R")
```

# Set up simulation

The following simulation assumes $\beta \sim N(0,betasd^2)$ and with $\hat{\beta}$ having standard deviation 1.
```{r}
timed_sims = function(ash.args,nsim=20,nmin = 100,nmax = 1000,betasd=1){
  n=10^seq(log10(nmin),log10(nmax),length=nsim)
  elapsed.time= rep(0,nsim)
  for(i in 1:nsim){
    set.seed(i)
    betahat = rnorm(n[i],sd=sqrt(1+betasd^2))
    elapsed.time[i] = system.time(do.call(ashr::ash,args = modifyList(ash.args,list(betahat = betahat, sebetahat=1))))[3]
  }
  return(data.frame(elapsed.time=elapsed.time,seed=1:nsim,n = n))
}
```

Now run a simulation study, for $n$ (number of tests; $p$ in paper) in range 10 to 100,000.
Note that the warnings are being generated by the EM algorithm in big problems due to lack of convergence.

```{r}
  df=data.frame()
  for(method in c("fdr","shrink")){
    for(optmethod in c("mixIP","cxxMixSquarem")){
      df = rbind(df, data.frame(method=method, optmethod=optmethod,  timed_sims(list(method=method,optmethod=optmethod),nsim=50,nmin=10,nmax=100000)))
    }
  }
  
```

# Results

Now plot the time as a function of $n$
```{r}
  ggplot2::qplot(x=n,y=elapsed.time,data=df,col=optmethod,facets=.~method,ylab="elapsed time (s)")
```

And a zoom-in on "small" problems with $n<5000$.
```{r}
  ggplot2::qplot(x=n,y=elapsed.time,data=dplyr::filter(df,n<5000),col=optmethod,facets=.~method,ylab="elapsed time (s)")
```

# Summary 

The IP method clearly scales to large problem much better than EM. It is faster, and also more reliable (sometimes reaches higher log-likelihood than EM, never smaller); see
[checkIP.html](checkIP.html). 

However, for small problems (n<5000) the EM is adequate for many practical purposes, solving within a few seconds. This is particularly true for the penalty term (method=fdr), which
 helps the EM converge, presumably because it is helping identifiability, removing the large flat parts of the likelihood objective function. 
 
Indeed, for small problems with the penalty term (n<2000) EM with squarem acceleration is a little faster in these comparisons than IP.




## Session information

```{r info}
sessionInfo()
```

