---
title: "Post-selection CI example, assymetric, not unimodal at 0"
author: "Matthew Stephens"
date: 2016-05-09
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=FALSE}
source("chunk-options.R")
```


## Introduction

This example comes from Efron (2008) p16 when examining the false coverage rate (FCR). 
I selected this example because the distribution of the non-zero effect is
highly assymetric and not at all unimodal at zero, both issues a referee asked me
to elaborate on. Specifically, the distribution of the non-zero effects is N(-3,1).
Here I simulate data, and apply ash (with the "halfuniform" option to allow for asymmetric g).

```{r}
  set.seed(10)
  nsamp=10000
  altmean=-3
  mu0 = rep(0,nsamp)
  mu1 = rnorm(nsamp,altmean,1)
  comp = rbinom(nsamp,1,0.1)
  mu = ifelse(comp==0,mu0,mu1)
  z = rnorm(nsamp,mu,1)
  
  res.ash = ashr::ash(z,1,mixcompdist="halfuniform")

```


We can also run ash with the "true" g, to allow us to compare the lfsr, lfdr etc. 
```{r}
  true.g = ashr::normalmix(c(0.9,0.1),c(0,-3),c(0,1))
  res.ash.true = ashr::ash(z,1,g=true.g,fixg=TRUE)
```

Here we can see how the partition of $z$ scores compares with the truth. Note the effect of the unimodal assumption is to extend the inferred alternative distribution toward 0.
```{r}
  source("../R/nullalthist.R")
  par(mfcol=c(2,1))
  nullalthist(z,lfdr=res.ash.true$lfdr,main="true partition")
  nullalthist(z,lfdr=res.ash$lfdr,main="inferred partition")
```

Comparing the inferred Posterior Means, lfdr, and lfsr with the true values of these quantities, we find reassuringly good correspondence.
```{r}
  par(mfcol=c(1,1))
  plot(res.ash.true$PosteriorMean,res.ash$PosteriorMean,xlab="Truth", ylab="ash.hu",main="Posterior Mean (inferred vs truth)",xlim=c(-6,1),ylim=c(-6,1))
  abline(a=0,b=1,col=2,lwd=3)
  
    plot(res.ash.true$lfdr,res.ash$lfdr,xlab="Truth", ylab="ash.hu",main="lfdr (inferred vs truth)",xlim=c(0,1),ylim=c(0,1))
  abline(a=0,b=1,col=2,lwd=3)
  
  plot(res.ash.true$lfsr,res.ash$lfsr,xlab="Truth", ylab="ash.hu",main="lfsr (inferred vs truth)",xlim=c(0,1),ylim=c(0,1))
  abline(a=0,b=1,col=2,lwd=3)
  
```

We can also run qvalue and locfdr. We see that locfdr perhaps performs a bit better than ash for the decomposition here, but the estimated local fdrs are pretty similar. Here qvalue does less well because of the asymmetry which we didn't take account of.
```{r}
library(qvalue)
library(locfdr)
res.locfdr = locfdr(z,nulltype = 0)
res.qvalue = qvalue(p=pchisq(z^2,df=1,lower.tail = FALSE))  
#res.pos.qvalue = qvalue(p=pchisq(z[z>0]^2,df=1,lower.tail = FALSE))  
#res.neg.qvalue = qvalue(p=pchisq(z[z<0]^2,df=1,lower.tail = FALSE))  
#df1= data.frame(Truth = res.ash.true$lfdr,Estimate=res.locfdr$fdr,Method="locfdr")
#df2= data.frame(Truth = res.ash.true$lfdr,Estimate=res.ash$lfdr,Method="ash.hu")
#df3 =data.frame(Truth = res.ash.true$lfdr,Estimate=res.qvalue$lfdr,Method="qvalue")
#df4 =data.frame(Truth = res.ash.true$lfdr[z<0],Estimate=res.neg.qvalue$lfdr,Method="qvalue (-)")
#df5 =data.frame(Truth = res.ash.true$lfdr[z>0],Estimate=res.pos.qvalue$lfdr,Method="qvalue (+)")

#df = rbind(df1,df2,df3,df4,df5)
#ggplot2::qplot(data=df,x=Truth,y=Estimate,facets= .~Method)

par(mfrow=c(1,3))
plot(res.ash.true$lfdr,res.ash$lfdr,xlab="Truth (lfdr)", ylab="ash.hu",main="ash.hu",xlim=c(0,1),ylim=c(0,1))
abline(a=0,b=1,col=2,lwd=3)
plot(res.ash.true$lfdr,res.locfdr$fdr,xlab="Truth (lfdr)", ylab="Estimate",main="locfdr",xlim=c(0,1),ylim=c(0,1))
abline(a=0,b=1,col=2,lwd=3)
plot(res.ash.true$lfdr,res.qvalue$lfdr,xlab="Truth (lfdr)", ylab="Estimate",main="qvalue",xlim=c(0,1),ylim=c(0,1))
abline(a=0,b=1,col=2,lwd=3)

```


# Uniform tail curtails Credible Intervals

The following plot compares the (symmetric-tail) 95% CIs from ash (red) for the "significant" observations with Bayes rule (green), similar to Figure 8 from Efron. Note that the lower 97.5% point is pretty accurate, but the upper 97.5% point is curtailed - presumably due, at least in part, to the short tails of the uniform mixture.
```{r}
  CImatrix= ashr::ashci(res.ash,level=0.95)
  BayesComparePlot=function(CImatrix,altmean=-3,...){
    plot(z,mu,xlim=c(-8,0),...)
    points(z[CImatrix[,1]],CImatrix[,5],col="red")
    points(z[CImatrix[,1]],CImatrix[,4],col="red")

    fdr = 0.9*dnorm(z)/(0.9*dnorm(z)+0.1*dnorm(z,altmean,sqrt(2)))
    o=order(z)
    upper = ifelse(fdr[o]<0.025,(z[o]+altmean)/2+qnorm(0.975+fdr[o])/sqrt(2),0)
    lines(z[o],upper,col="green",lwd=2)
    lines(z[o],(z[o]+altmean)/2-qnorm(0.975)/sqrt(2),col="green",lwd=2)
    abline(v=max(z[fdr<0.05]))
  }
  BayesComparePlot(CImatrix,main="CIs for highly asymmetric and non-unimodal-at-zero data")
```




## Variational version

Although not a focus of the paper, ash does have an option to do variational inference for the mixture components (with a Dirichlet prior). In practice this approach usually
ends up spreading the posterior mass up more among the mixture components. It
seemed plausible that this might lead to slightly less extreme tail behaviour than
above (because the model will put a little more weight on the uniforms with larger variance, which are essentially set to zero in the above).

```{r}
  res.ash.VB = ashr::ash(z,1,mixcompdist="halfuniform",optmethod="mixVBEM")
  CImatrix.VB= ashr::ashci(res.ash.VB,level=0.95)
```

Again, we can compare results with Bayes rule
```{r}
  BayesComparePlot(CImatrix.VB, main="CIs for highly asymmetric and non-unimodal-at-zero data \n Variational Version")
```


## Session information

```{r info}
sessionInfo()
```

